{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fb140897",
      "metadata": {
        "id": "fb140897"
      },
      "source": [
        "# Extract Narrations and Audio Captions Tutorial\n",
        "\n",
        "In this tutorial, you will be extracting video narrations through an auto-narration model, LaViLa, as well as audio captions through speech-to-text model, WhisperX. Finally, you will be able to interact with the extracted narrations and captions using langchain.\n",
        "\n",
        "### Notebook stuck?\n",
        "Note that because of Jupyter issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Install Project Aria Tools\n",
        "Run the following cell to install Project Aria Tools for reading Aria recordings in .vrs format"
      ],
      "metadata": {
        "id": "l5LlycOs75bf"
      },
      "id": "l5LlycOs75bf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifics for Google Colab\n",
        "google_colab_env = 'google.colab' in str(get_ipython())\n",
        "print(\"Running from Google Colab, installing projectaria_tools\")\n",
        "!pip install projectaria-tools"
      ],
      "metadata": {
        "id": "Yt5CoQlp8Cxw"
      },
      "id": "Yt5CoQlp8Cxw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Prepare an Aria recording\n",
        "\n",
        "#### (Option 1) Download a sample data\n",
        "We recommend running this tutorial with this small scale sample data first for testing out the dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tv0I3ajm7TyH"
      },
      "id": "Tv0I3ajm7TyH"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O -J -L \"https://github.com/facebookresearch/projectaria_tools/raw/main/data/mps_sample/sample.vrs\"\n",
        "vrsfile = \"sample.vrs\"\n",
        "print(f\"INFO: vrsfile set to {vrsfile}\")"
      ],
      "metadata": {
        "id": "WKyjF8fF7tA_"
      },
      "id": "WKyjF8fF7tA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Option 2) Prepare your collected Aria recording\n",
        "We will set the vrsfile path to your collected Aria recording.\n",
        "\n",
        "Upload your Aria recording in your Google Drive before running the cell.\n",
        "\n",
        "Here, we assume it is uploaded to **`My Drive/aria/recording.vrs`**\n",
        "\n",
        "*(You can check the content of the mounted drive by running `!ls \"/content/drive/My Drive/\"` in a cell.)*"
      ],
      "metadata": {
        "id": "vZLboooc7QLQ"
      },
      "id": "vZLboooc7QLQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "my_vrs_file_path = 'aria/recording.vrs'\n",
        "vrsfile = os.path.join(\"/content/drive/My Drive/\", my_vrs_file_path)\n",
        "print(f\"INFO: vrsfile set to {vrsfile}\")"
      ],
      "metadata": {
        "id": "vnjjSGXlc4-i"
      },
      "id": "vnjjSGXlc4-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8196ad05",
      "metadata": {
        "id": "8196ad05"
      },
      "source": [
        "## Step 3. Create data provider\n",
        "\n",
        "Create projectaria data_provider so you can load the content of the vrs file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb04b53b",
      "metadata": {
        "id": "fb04b53b"
      },
      "outputs": [],
      "source": [
        "from projectaria_tools.core import data_provider, calibration\n",
        "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
        "from projectaria_tools.core.stream_id import RecordableTypeId, StreamId\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(f\"Creating data provider from {vrsfile}\")\n",
        "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
        "if not provider:\n",
        "    print(\"Invalid vrs data provider\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5033225",
      "metadata": {
        "id": "c5033225"
      },
      "source": [
        "## Step 4. Display VRS rgb content in thumbnail images\n",
        "\n",
        "Goals:\n",
        "- Summarize a VRS using 10 image side by side, to visually inspect the collected data.\n",
        "\n",
        "Key learnings:\n",
        "- Image streams are identified with a Unique Identifier: stream_id\n",
        "- Image frames are identified with timestamps\n",
        "- PIL images can be created from Numpy array\n",
        "\n",
        "Customization\n",
        "- To change the number of sampled images, change the variable `sample_count` to a desired number.\n",
        "- To change the thumbnail size, change the variable `resize_ratio` to a desired value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933725b6",
      "metadata": {
        "id": "933725b6"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "sample_count = 10\n",
        "resize_ratio = 10\n",
        "\n",
        "rgb_stream_id = StreamId(\"214-1\")\n",
        "\n",
        "# Retrieve image size for the RGB stream\n",
        "time_domain = TimeDomain.DEVICE_TIME  # query data based on host time\n",
        "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
        "\n",
        "# Retrieve Start and End time for the given Sensor Stream Id\n",
        "start_time = provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
        "end_time = provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
        "\n",
        "image_config = provider.get_image_configuration(rgb_stream_id)\n",
        "width = image_config.image_width\n",
        "height = image_config.image_height\n",
        "\n",
        "thumbnail = newImage = Image.new(\n",
        "    \"RGB\", (int(width * sample_count / resize_ratio), int(height / resize_ratio))\n",
        ")\n",
        "current_width = 0\n",
        "\n",
        "\n",
        "# Samples 10 timestamps\n",
        "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
        "for sample in tqdm(sample_timestamps):\n",
        "    image_tuple = provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
        "    image_array = image_tuple[0].to_numpy_array()\n",
        "    image = Image.fromarray(image_array)\n",
        "    new_size = (\n",
        "        int(image.size[0] / resize_ratio),\n",
        "        int(image.size[1] / resize_ratio),\n",
        "    )\n",
        "    image = image.resize(new_size).rotate(-90)\n",
        "    thumbnail.paste(image, (current_width, 0))\n",
        "    current_width = int(current_width + width / resize_ratio)\n",
        "\n",
        "from IPython.display import Image\n",
        "display(thumbnail)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Prepare Pytorch Data Loader for Auto-Narration\n",
        "\n",
        "Here, we will be creating a pytorch data loader that outputs batches of video snippets in order to run the LaViLa auto-narration model.\n",
        "\n",
        "A **snippet** consists of a series of frames captured over a brief time span, which we will refer to as **snippet duration**.\n",
        "\n",
        "#### Step 5-1. Define Dataset"
      ],
      "metadata": {
        "id": "nFkss4vlaWxh"
      },
      "id": "nFkss4vlaWxh"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms._transforms_video as transforms_video\n",
        "import torch.nn as nn\n",
        "\n",
        "class RGBSnippetDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 start_time: float, # start time in the video for sampling data\n",
        "                 end_time: float, # end time in the video for sampling data\n",
        "                 snippet_dur_sec: int, # snippet duration\n",
        "                 frames_per_snippet: int, # number of frames per snippet\n",
        "                 transform=None\n",
        "    ):\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time\n",
        "        self.snippet_dur = snippet_dur_sec * 1000000000 # duration of a snippet in nano seconds\n",
        "        self.frames_per_snippet = frames_per_snippet # number of frames per snippet\n",
        "        self.stride_ns = int(self.snippet_dur//frames_per_snippet)\n",
        "        self.num_snippets = int((end_time - start_time) // self.snippet_dur)\n",
        "        self.snippet_starts = np.arange(start_time, start_time + self.snippet_dur * self.num_snippets, self.snippet_dur)\n",
        "\n",
        "        # Precompute timestamps for each snippet\n",
        "        self.all_frame_timestamps = [np.arange(snippet_start, snippet_start + self.snippet_dur, self.stride_ns) for snippet_start in self.snippet_starts]\n",
        "\n",
        "        self.rgb_stream_id = rgb_stream_id\n",
        "        self.time_domain = time_domain\n",
        "        self.option = option\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_snippets\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # returns a snippet\n",
        "\n",
        "        # get timestamps of frames that belong to the current snippet idx\n",
        "        frame_timestamps = self.all_frame_timestamps[idx]\n",
        "\n",
        "        # read frames from the data provider and append to frame_list\n",
        "        frame_list = []\n",
        "        for timestamp in frame_timestamps:\n",
        "            image_tuple = provider.get_image_data_by_time_ns(self.rgb_stream_id, int(timestamp), self.time_domain, self.option)\n",
        "            image_array = image_tuple[0].to_numpy_array()\n",
        "            frame_list.append(image_array)\n",
        "\n",
        "        # append a set of images to a snippet\n",
        "        frames = [torch.tensor(frame, dtype=torch.float32) for frame in frame_list]\n",
        "        frames = torch.stack(frames, dim=0)\n",
        "\n",
        "        if self.transform:\n",
        "          frames = self.transform(frames)\n",
        "\n",
        "        # return snippet start time and end time\n",
        "        snippet_start = self.snippet_starts[idx]\n",
        "        snippet_end = snippet_start + self.snippet_dur\n",
        "\n",
        "        return frames, snippet_start, snippet_end\n",
        "\n",
        "class Permute(nn.Module):\n",
        "    \"\"\"\n",
        "    Permutation as an op\n",
        "    \"\"\"\n",
        "    def __init__(self, ordering):\n",
        "        super().__init__()\n",
        "        self.ordering = ordering\n",
        "\n",
        "    def forward(self, frames):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            frames in some ordering, by default (C, T, H, W)\n",
        "        Returns:\n",
        "            frames in the ordering that was specified\n",
        "        \"\"\"\n",
        "        return frames.permute(self.ordering)"
      ],
      "metadata": {
        "id": "Kee0HuCgKUy_"
      },
      "id": "Kee0HuCgKUy_",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5-2. Construct Data Loader\n",
        "Here you can set batch size (`batch_size`) as well as customize start time, end_time for running auto-narration."
      ],
      "metadata": {
        "id": "N7p7uCJZKPNm"
      },
      "id": "N7p7uCJZKPNm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Start and End time for the given Sensor Stream Id\n",
        "start_time = provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
        "end_time = provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
        "\n",
        "batch_size = 2 # batch size in dataloader (Decrease/increase based on the GPU memory)\n",
        "image_size = 224  # image size after resizing (Do not change for LaViLa)\n",
        "snippet_dur_sec = 2  # duration of a snippet (We recommend values between 1-10.)\n",
        "frames_per_snippet = 4  # number of frames per snippet (Do not change for LaViLa)\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    Permute([3, 0, 1, 2]),  # T H W C -> C T H W\n",
        "    transforms.Resize(image_size),\n",
        "    transforms_video.NormalizeVideo(mean=[108.3272985, 116.7460125, 104.09373615000001], std=[68.5005327, 66.6321579, 70.32316305]),\n",
        "])\n",
        "rgb_snippet_dataset = RGBSnippetDataset(start_time, end_time, snippet_dur_sec=snippet_dur_sec, frames_per_snippet=frames_per_snippet, transform=val_transform)\n",
        "snippet_dataloader = DataLoader(rgb_snippet_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "gWvmKM_nS0Ri"
      },
      "id": "gWvmKM_nS0Ri",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6. Install LaViLa auto-narration library\n",
        "Now that the data is prepared, let's install LaViLa library."
      ],
      "metadata": {
        "id": "ixrGS0J0afaE"
      },
      "id": "ixrGS0J0afaE"
    },
    {
      "cell_type": "code",
      "source": [
        "# install LaViLa as dependency\n",
        "!pip install git+https://github.com/zhaoyang-lv/LaViLa"
      ],
      "metadata": {
        "id": "7vLd1utsaqAr"
      },
      "id": "7vLd1utsaqAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7. Define helper functions for LaViLa\n",
        "\n",
        "Run the following cell for defining helper functions for (1) loading pre-trained models and tokenizers, (2) decoding generated tokens, and (3) run model on a batch of snippets."
      ],
      "metadata": {
        "id": "9xPZVr6eMIsp"
      },
      "id": "9xPZVr6eMIsp"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "from lavila.models.models import VCLM_OPENAI_TIMESFORMER_LARGE_336PX_GPT2_XL, VCLM_OPENAI_TIMESFORMER_BASE_GPT2\n",
        "from lavila.models.tokenizer import MyGPT2Tokenizer\n",
        "\n",
        "DEFAULT_CHECKPOINT = 'vclm_openai_timesformer_base_gpt2_base.pt_ego4d.jobid_319630.ep_0002.md5sum_68a71f.pth'\n",
        "# DEFAULT_CHECKPOINT = 'vclm_openai_timesformer_large_336px_gpt2_xl.pt_ego4d.jobid_246897.ep_0003.md5sum_443263.pth'\n",
        "\n",
        "def load_models_and_transforms(num_frames=4, ckpt_name=DEFAULT_CHECKPOINT, device='cpu'):\n",
        "    '''\n",
        "    Helper function for loading oading pre-trained models and tokenizers\n",
        "    '''\n",
        "    ckpt_path = os.path.join('lavila/modelzoo/', ckpt_name)\n",
        "    print(f\"ckpt_path: {os.path.abspath(ckpt_path)}\")\n",
        "    os.makedirs('lavila/modelzoo/', exist_ok=True)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print('downloading model to {}'.format(ckpt_path))\n",
        "        urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/lavila/checkpoints/narrator/{}'.format(ckpt_name), ckpt_path)\n",
        "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "    state_dict = OrderedDict()\n",
        "    for k, v in ckpt['state_dict'].items():\n",
        "        state_dict[k.replace('module.', '')] = v\n",
        "\n",
        "    # instantiate the model, and load the pre-trained weights\n",
        "    # model = VCLM_OPENAI_TIMESFORMER_LARGE_336PX_GPT2_XL(\n",
        "    model = VCLM_OPENAI_TIMESFORMER_BASE_GPT2(\n",
        "        text_use_cls_token=False,\n",
        "        project_embed_dim=256,\n",
        "        gated_xattn=True,\n",
        "        timesformer_gated_xattn=False,\n",
        "        freeze_lm_vclm=False,      # we use model.eval() anyway\n",
        "        freeze_visual_vclm=False,  # we use model.eval() anyway\n",
        "        num_frames=num_frames,\n",
        "        drop_path_rate=0.\n",
        "    )\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    device_type_str = device.type if isinstance(device, torch.device) else device\n",
        "    if device_type_str != 'cpu':\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = MyGPT2Tokenizer('gpt2', add_bos=True)\n",
        "    #tokenizer = MyGPT2Tokenizer('gpt2-xl', add_bos=True)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def decode_one(generated_ids, tokenizer):\n",
        "    '''\n",
        "    Helper function for decoding generated tokens.\n",
        "    '''\n",
        "    # get the index of <EOS>\n",
        "    if tokenizer.eos_token_id == tokenizer.bos_token_id:\n",
        "        if tokenizer.eos_token_id in generated_ids[1:].tolist():\n",
        "            eos_id = generated_ids[1:].tolist().index(tokenizer.eos_token_id) + 1\n",
        "        else:\n",
        "            eos_id = len(generated_ids.tolist()) - 1\n",
        "    elif tokenizer.eos_token_id in generated_ids.tolist():\n",
        "        eos_id = generated_ids.tolist().index(tokenizer.eos_token_id)\n",
        "    else:\n",
        "        eos_id = len(generated_ids.tolist()) - 1\n",
        "    generated_text_str = tokenizer.tokenizer.decode(generated_ids[1:eos_id].tolist())\n",
        "    return generated_text_str\n",
        "\n",
        "\n",
        "def run_model_on_snippets(\n",
        "    frames, model, tokenizer, device=\"cpu\", narration_max_sentences=5\n",
        "):\n",
        "    '''\n",
        "    Function for running the LaViLa model on batches of snippets.\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(frames)\n",
        "        generated_text_ids, ppls = model.generate(\n",
        "            image_features,\n",
        "            tokenizer,\n",
        "            target=None,  # free-form generation\n",
        "            max_text_length=77,\n",
        "            top_k=None,\n",
        "            top_p=0.95,   # nucleus sampling\n",
        "            num_return_sequences=narration_max_sentences,  # number of candidates: 10\n",
        "            temperature=0.7,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    output_narration = []\n",
        "    for j in range(generated_text_ids.shape[0] // narration_max_sentences):\n",
        "        cur_output_narration = []\n",
        "        for k in range(narration_max_sentences):\n",
        "            jj = j * narration_max_sentences + k\n",
        "            generated_text_str = decode_one(generated_text_ids[jj], tokenizer)\n",
        "            generated_text_str = generated_text_str.strip()\n",
        "            generated_text_str = generated_text_str.replace(\"#c c\", \"#C C\")\n",
        "            if generated_text_str in cur_output_narration:\n",
        "                continue\n",
        "            if generated_text_str.endswith('the'):\n",
        "                # skip incomplete sentences\n",
        "                continue\n",
        "            cur_output_narration.append(generated_text_str)\n",
        "        output_narration.append(cur_output_narration) # list of size B (batch size)\n",
        "    return output_narration"
      ],
      "metadata": {
        "id": "qFnuvPo1b3Dk"
      },
      "id": "qFnuvPo1b3Dk",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8. Run LaViLa inference over vrs file\n",
        "Let's load the pre-traiend model and tokenizer\n"
      ],
      "metadata": {
        "id": "mJ7qlnBevC06"
      },
      "id": "mJ7qlnBevC06"
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pre-traiend model and tokenizer.\n",
        "model, tokenizer = load_models_and_transforms(num_frames=4)\n",
        "\n",
        "# this is where the generated narration will be stored\n",
        "narrations_dict = {\n",
        "    'start_time_ns': [],\n",
        "    'end_time_ns': [],\n",
        "    'narration': [],\n",
        "}\n",
        "\n",
        "# use gpu if available\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "\n",
        "for idx, (frames, st_ns, ed_ns) in enumerate(snippet_dataloader):\n",
        "  if torch.cuda.is_available():\n",
        "    frames = frames.cuda()\n",
        "  # run inference over a batch of snippet\n",
        "  output_narration = run_model_on_snippets(frames, model, tokenizer)\n",
        "  # store results\n",
        "  narrations_dict['start_time_ns'].extend(st_ns.numpy().tolist())\n",
        "  narrations_dict['end_time_ns'].extend(ed_ns.numpy().tolist())\n",
        "  narrations_dict['narration'].extend(output_narration)"
      ],
      "metadata": {
        "id": "d7C99NViqb_a"
      },
      "id": "d7C99NViqb_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9. Display the auto-narration results and save to csv file\n",
        "Make sure to change `narration_save_path` to your desired location!"
      ],
      "metadata": {
        "id": "z3pRVNe5vPIL"
      },
      "id": "z3pRVNe5vPIL"
    },
    {
      "cell_type": "code",
      "source": [
        "narration_save_path = os.path.join(os.path.dirname(vrsfile), 'auto_narration.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(narrations_dict)\n",
        "df.to_csv(narration_save_path)\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "E6nUZYKurDpO",
        "outputId": "8164a676-94d8-4c90-8d34-2b9674d60947"
      },
      "id": "E6nUZYKurDpO",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   start_time_ns   end_time_ns  \\\n",
              "0   148502526450  150502526450   \n",
              "1   150502526450  152502526450   \n",
              "2   152502526450  154502526450   \n",
              "3   154502526450  156502526450   \n",
              "4   156502526450  158502526450   \n",
              "5   158502526450  160502526450   \n",
              "\n",
              "                                           narration  \n",
              "0  [#C C stares at the ceiling, #C C looks around...  \n",
              "1  [#C C looks around the house, #C C looks aroun...  \n",
              "2       [#C C adjusts the camera, #C C looks around]  \n",
              "3                                [#C C looks around]  \n",
              "4   [#C C stands beside the door, #C C looks around]  \n",
              "5  [#C C looks at the wall, #C C looks around, #C...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time_ns</th>\n",
              "      <th>end_time_ns</th>\n",
              "      <th>narration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148502526450</td>\n",
              "      <td>150502526450</td>\n",
              "      <td>[#C C stares at the ceiling, #C C looks around...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150502526450</td>\n",
              "      <td>152502526450</td>\n",
              "      <td>[#C C looks around the house, #C C looks aroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>152502526450</td>\n",
              "      <td>154502526450</td>\n",
              "      <td>[#C C adjusts the camera, #C C looks around]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154502526450</td>\n",
              "      <td>156502526450</td>\n",
              "      <td>[#C C looks around]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156502526450</td>\n",
              "      <td>158502526450</td>\n",
              "      <td>[#C C stands beside the door, #C C looks around]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>158502526450</td>\n",
              "      <td>160502526450</td>\n",
              "      <td>[#C C looks at the wall, #C C looks around, #C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-709b9be8-de5d-4fa3-b612-f5fb6a346ef4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9b5fcf7-00b2-428b-b18a-610e592c61d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9b5fcf7-00b2-428b-b18a-610e592c61d4')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9b5fcf7-00b2-428b-b18a-610e592c61d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10. Build VRS Tool to extract .wav file for audio captioning\n",
        "Whisper X can be run on .wav file. We need to install VRSTool for extracting .wav file from .vrs file."
      ],
      "metadata": {
        "id": "upn5Is4AhRpm"
      },
      "id": "upn5Is4AhRpm"
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "# Install VRS dependencies\n",
        "!sudo apt-get install cmake git ninja-build ccache libgtest-dev libfmt-dev libturbojpeg-dev libpng-dev\n",
        "!sudo apt-get install liblz4-dev libzstd-dev libxxhash-dev\n",
        "!sudo apt-get install libboost-system-dev libboost-filesystem-dev libboost-thread-dev libboost-chrono-dev libboost-date-time-dev\n",
        "# Install build dependencies\n",
        "!sudo apt-get install -y cmake ninja-build\n",
        "\n",
        "#clone and build\n",
        "!git clone https://github.com/facebookresearch/vrs.git\n",
        "!cmake -S vrs -B vrs/build -G Ninja\n",
        "!cd vrs/build; ninja all vrs\n"
      ],
      "metadata": {
        "id": "N5gLce1Py09s"
      },
      "id": "N5gLce1Py09s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12. Extract .wav file from VRS file\n",
        "Now that VRSTool is installed, let's extract .wav file from .vrs file."
      ],
      "metadata": {
        "id": "jXN0HNZ5iGOy"
      },
      "id": "jXN0HNZ5iGOy"
    },
    {
      "cell_type": "code",
      "source": [
        "!./vrs/build/tools/vrs/vrs extract-audio sample.vrs --to ."
      ],
      "metadata": {
        "id": "lSSdMm3Tz31_"
      },
      "id": "lSSdMm3Tz31_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 13. Install Whisper X\n",
        "We have input data ready for Whisper X. Let's install the library."
      ],
      "metadata": {
        "id": "Wleqb4JLkNNE"
      },
      "id": "Wleqb4JLkNNE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git"
      ],
      "metadata": {
        "id": "0Su4GYTFkPKU"
      },
      "id": "0Su4GYTFkPKU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14. Define helper functions for Whisper X\n",
        "Let's define some helper functions for Whisper X, this include a postprocessing function and a function to align the output to the timestamps."
      ],
      "metadata": {
        "id": "1a2IoR3CiMxl"
      },
      "id": "1a2IoR3CiMxl"
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import whisperx\n",
        "import tqdm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "def asr_tokens_to_csv(\n",
        "    word_segments,\n",
        "    token_csv_folder: str,\n",
        "    starting_timestamp_s: float = 0.0,\n",
        "):\n",
        "    # post process the output asr file to extract only the minimal needed content\n",
        "\n",
        "    df = pd.DataFrame.from_dict(word_segments)\n",
        "    os.makedirs(token_csv_folder, exist_ok=True)\n",
        "\n",
        "    # write to wav domain:\n",
        "    s_to_ms = int(1e3)\n",
        "    df[\"start\"] = (df[\"start\"] * s_to_ms).astype(\"int64\")\n",
        "    df[\"end\"] = (df[\"end\"] * s_to_ms).astype(\"int64\")\n",
        "    df_speech_wav = df.rename(\n",
        "        columns={\"start\": \"startTime_ms\", \"end\": \"endTime_ms\", \"text\": \"written\"},\n",
        "    )\n",
        "    df_speech_wav.to_csv(\n",
        "        osp.join(token_csv_folder, \"speech.csv\"), index=False, header=True\n",
        "    )\n",
        "\n",
        "    # Update ASR ms time to Aria ns time\n",
        "    s_to_ns = int(1e9)\n",
        "    ms_to_ns = int(1e6)\n",
        "    df[\"start\"] = (df[\"start\"] * ms_to_ns + starting_timestamp_s * s_to_ns).astype(\n",
        "        \"int64\"\n",
        "    )\n",
        "    df[\"end\"] = (df[\"end\"] * ms_to_ns + starting_timestamp_s * s_to_ns).astype(\"int64\")\n",
        "\n",
        "    df_aria_domain = df.rename(\n",
        "        columns={\"start\": \"startTime_ms\", \"end\": \"endTime_ms\", \"text\": \"written\"},\n",
        "    )\n",
        "    df_aria_domain.to_csv(\n",
        "        osp.join(token_csv_folder, \"speech_aria_domain.csv\"), index=False, header=True\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Generate speech.csv & speech_aria_domain.csv to {token_csv_folder}\")\n",
        "\n",
        "\n",
        "def run_whisperx_aria_wav(\n",
        "    model,\n",
        "    file_path: str,\n",
        "    output_folder: str = \"\",\n",
        "    batch_size = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run whisperx model on .wav file extracted from VRS file\n",
        "    \"\"\"\n",
        "    starting_timestamp = file_path.split(\"-\")[-1].replace(\".wav\", \"\")\n",
        "    starting_timestamp = float(starting_timestamp)\n",
        "    logging.info(\"Aria Starting timestamp: {:0.3f}\".format(starting_timestamp))\n",
        "\n",
        "    logging.info(f\"Transcribe the speech from wav file {file_path}.\")\n",
        "    result = model.transcribe(file_path, batch_size=batch_size)\n",
        "\n",
        "    try:\n",
        "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "\n",
        "        result_aligned = whisperx.align(\n",
        "            result[\"segments\"], model_a, metadata, file_path, device\n",
        "        )\n",
        "\n",
        "        asr_tokens_to_csv(\n",
        "            word_segments=result_aligned[\"word_segments\"],\n",
        "            token_csv_folder=output_folder,\n",
        "            starting_timestamp_s=starting_timestamp,\n",
        "        )\n",
        "\n",
        "    except Exception as err:\n",
        "        logging.warning(f\"Cannot process {file_path} because {err}. Skip this recording.\")"
      ],
      "metadata": {
        "id": "_wavV43tiSwM"
      },
      "id": "_wavV43tiSwM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 15 Run Whisper X\n",
        "Finally, let's run Whisper X on the .wav file that we extracted.\n",
        "\n",
        "Make sure to set the `whisper_x_output_folder` to desired location. The resulting file name is `speech_aria_domain.csv`."
      ],
      "metadata": {
        "id": "C3q0o21rRIbm"
      },
      "id": "C3q0o21rRIbm"
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '231-1-0000-148.518.wav'\n",
        "whisper_x_output_folder = \".\"\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "batch_size = 16 # reduce if low on GPU mem, or keep it None\n",
        "provider = run_whisperx_aria_wav(model, audio_file, output_folder=whisper_x_output_folder, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ZODeWa-ilKMJ"
      },
      "id": "ZODeWa-ilKMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16. Install Langchain"
      ],
      "metadata": {
        "id": "SBANXKSPR1eP"
      },
      "id": "SBANXKSPR1eP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "t25suH5HR5jb"
      },
      "id": "t25suH5HR5jb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Install OpenAI to use with Langchain"
      ],
      "metadata": {
        "id": "temVxjVVWBw3"
      },
      "id": "temVxjVVWBw3"
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "r6QEuPO0WFgC"
      },
      "id": "r6QEuPO0WFgC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Example) Summaraize the narration result\n"
      ],
      "metadata": {
        "id": "Nb6m7nHzSBy9"
      },
      "id": "Nb6m7nHzSBy9"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "prompt_template = \"\"\" Write a concise summary (between 5 to 10 sentences) of the following text.\n",
        "The text is about my exhaustive timeline, where I am referred to as '#C C' or 'C C' or 'C'.\n",
        "Please use first person pronoun (I) in the summary, instead of 'C' or 'C C'.\n",
        "Please keep in mind that some observations maybe incorrect as the timeline was machine-generated.\n",
        "\n",
        "Timeline:\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "TL'DR: \"\"\"\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key\"\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "loader = CSVLoader(file_path=narration_save_path)\n",
        "docs = loader.load()\n",
        "\n",
        "chain.run(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "igRz39kKSBES",
        "outputId": "c4f09b38-7d0e-42a1-c451-a50c88f250c7"
      },
      "id": "igRz39kKSBES",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The summary describes a series of events with timestamps and corresponding narrations. The events involve a character named C C who is observed looking around, adjusting the camera, staring at the ceiling, standing beside the door, and looking at various objects in the room and house.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) RGBDataset\n",
        "\n",
        "The RGBDataset is a simple image pytorch dataset designed for image-based models that operate on individual frames rather than snippet inputs. Use this dataset that process single frames."
      ],
      "metadata": {
        "id": "oV6Tn_WACKMF"
      },
      "id": "oV6Tn_WACKMF"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "# class RGBDataset(Dataset):\n",
        "#     def __init__(self, start_time, end_time, sample_count, transform=None):\n",
        "#         self.timestamps = np.linspace(start_time, end_time, sample_count)\n",
        "#         self.rgb_stream_id = StreamId(\"214-1\")\n",
        "#         self.time_domain = TimeDomain.DEVICE_TIME\n",
        "#         self.option = TimeQueryOptions.CLOSEST\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.timestamps)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         timestamp = self.timestamps[idx]\n",
        "#         image_tuple = provider.get_image_data_by_time_ns(self.rgb_stream_id, int(timestamp), self.time_domain, self.option)\n",
        "#         image_array = image_tuple[0].to_numpy_array()\n",
        "#         image = Image.fromarray(image_array).rotate(-90)\n",
        "#         if self.transform:\n",
        "#           image = self.transform(image)\n",
        "#         return timestamp, image\n",
        "\n",
        "# val_transform = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.ToTensor(),\n",
        "#   ])\n",
        "\n",
        "# rgb_dataset = RGBDataset(start_time, end_time, sample_count, transform=val_transform)\n",
        "# image_dataloader = DataLoader(rgb_dataset, batch_size=2, shuffle=False)\n",
        "# # Get the next batch of data\n",
        "# timestamp, image = next(iter(image_dataloader))"
      ],
      "metadata": {
        "id": "kvAqkUzdCLPt"
      },
      "id": "kvAqkUzdCLPt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "custom": {
      "cells": [],
      "metadata": {
        "fileHeader": "",
        "kernelspec": {
          "display_name": "Python 3 (ipykernel)",
          "language": "python",
          "name": "python3"
        },
        "language_info": {
          "codemirror_mode": {
            "name": "ipython",
            "version": 3
          },
          "file_extension": ".py",
          "mimetype": "text/x-python",
          "name": "python",
          "nbconvert_exporter": "python",
          "pygments_lexer": "ipython3",
          "version": "3.10.11"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 5
    },
    "indentAmount": 2,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}